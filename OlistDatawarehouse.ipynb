{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OlistDatawarehouse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOha4V/j+qVUf/E2bdFVYEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoFernanndes/portfolio/blob/master/OlistDatawarehouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T6E1q8QkZvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15980e8-9d10-4c44-b51b-084591aa0300"
      },
      "source": [
        "!pip install SQLAlchemy\n",
        "!apt install unixodbc-dev\n",
        "!pip install pyodbc"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.6/dist-packages (1.3.20)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unixodbc-dev is already the newest version (2.3.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Requirement already satisfied: pyodbc in /usr/local/lib/python3.6/dist-packages (4.0.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VDuKNJVkxKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61d8d74-4a34-414d-e86f-5784419149fb"
      },
      "source": [
        "%%sh\n",
        "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
        "curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
        "sudo apt-get update\n",
        "sudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://packages.microsoft.com/ubuntu/16.04/prod xenial InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "msodbcsql17 is already the newest version (17.6.1.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "\r100   983  100   983    0     0   3425      0 --:--:-- --:--:-- --:--:--  3425\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    79  100    79    0     0    506      0 --:--:-- --:--:-- --:--:--   506\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqkntRJlke_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e548a50-ff81-4432-a3fd-c5bbf7ea5b6d"
      },
      "source": [
        "import pyodbc \n",
        "\n",
        "pyodbc.drivers()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ODBC Driver 17 for SQL Server']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85zNSdc3twc3"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "# carregando arquivo com as chaves para o banco de dados\r\n",
        "credentials = open('mssql_olist_data_keys.json')\r\n",
        "credentials = json.load(credentials)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehLu1cjQ4q8J"
      },
      "source": [
        "try:\n",
        "  conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\n",
        "                            SERVER = credentials['mssql_server'],\n",
        "                            DATABASE = credentials['mssql_database'],\n",
        "                            UID = credentials['mssql_uid'],\n",
        "                            PWD = credentials['mssql_pwd'])\n",
        "  \n",
        "\n",
        "  cursor = conexion.cursor()\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'customers_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE customers_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          customer_id VARCHAR(100),\n",
        "          customer_unique_id VARCHAR(100),\n",
        "          customer_zip_code_prefix int,\n",
        "          customer_city varchar(100),\n",
        "          customer_state varchar(100)\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'geolocation_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE geolocation_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          geolocation_zip_code_prefix INT,\n",
        "          geolocation_lat FLOAT,\n",
        "          geolocation_lng FLOAT,\n",
        "          geolocation_city VARCHAR(100),\n",
        "          geolocation_state VARCHAR(100)\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'order_items_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE order_items_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          order_id VARCHAR(100),\n",
        "          order_item_id INT,\n",
        "          product_id VARCHAR(100),\n",
        "          seller_id VARCHAR(100),\n",
        "          shipping_limit_date DATETIME,\n",
        "          price FLOAT,\n",
        "          freight_value FLOAT\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'order_payments_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE order_payments_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          order_id VARCHAR(100),\n",
        "          payment_sequential INT,\n",
        "          payment_type VARCHAR(100),\n",
        "          payment_installments INT,\n",
        "          payment_value FLOAT\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'order_reviews_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE order_reviews_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          review_id VARCHAR(100),\n",
        "          order_id VARCHAR(100),\n",
        "          review_score INT,\n",
        "          review_comment_title VARCHAR(100),\n",
        "          review_comment_message VARCHAR(100),\n",
        "          review_creation_date DATETIME,\n",
        "          review_answer_timestamp DATETIME        \n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'orders_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE orders_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          order_id VARCHAR(100),\n",
        "          customer_id VARCHAR(100),\n",
        "          order_status VARCHAR(100),\n",
        "          order_purchase_timestamp DATETIME,\n",
        "          order_approved_at DATETIME,\n",
        "          order_delivered_at DATETIME,\n",
        "          order_delivered_carrier_date DATETIME,\n",
        "          order_delivered_customer_date DATETIME,\n",
        "          order_estimated_delivery_date DATETIME\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'products_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE products_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          product_id VARCHAR(100),\n",
        "          product_category_name VARCHAR(100),\n",
        "          product_name_lenght FLOAT,\n",
        "          product_description_lenght FLOAT,\n",
        "          products_photos_qty FLOAT,\n",
        "          product_weight_g FLOAT,\n",
        "          product_lenght_cm FLOAT,\n",
        "          product_height_cm FLOAT,\n",
        "          product_width_cm FLOAT\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'sellers_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE sellers_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          seller_id VARCHAR(100),\n",
        "          seller_zip_code_prefix INT,\n",
        "          seller_city VARCHAR(100),\n",
        "          seller_state VARCHAR(100)\n",
        "      )\n",
        "      END; \n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'product_category_name_translation')\n",
        "      BEGIN\n",
        "        CREATE TABLE product_category_name_translation (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          product_category_name VARCHAR(100),\n",
        "          product_category_name_english VARCHAR(100)\n",
        "      )\n",
        "      END; \n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  conexion.commit()\n",
        "  conexion.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Ocorreu um erro na tentativa de conexão: \", e)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1kV0gLiGksb",
        "outputId": "46147517-2458-45aa-b2a0-9153c1622d24"
      },
      "source": [
        "try:\n",
        "  conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\n",
        "                          SERVER = credentials['mssql_server'],\n",
        "                          DATABASE = credentials['mssql_database'],\n",
        "                          UID = credentials['mssql_uid'],\n",
        "                          PWD = credentials['mssql_pwd'])\n",
        "\n",
        "  cursor = conexion.cursor()\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      SELECT * FROM INFORMATION_SCHEMA.TABLES;\n",
        "      \"\"\"\n",
        "      )\n",
        "\n",
        "  tables = cursor.fetchall()\n",
        "  conexion.close()\n",
        "\n",
        "  for table in tables:\n",
        "    print(table)\n",
        "\n",
        "\n",
        "    \n",
        "except Exception as e:\n",
        "  print(\"Ocorreu um erro na tentativa de conexão: \", e)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('olist_data', 'dbo', 'geolocation_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'order_items_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'order_payments_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'order_reviews_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'orders_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'products_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'sellers_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'product_category_name_translation', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'customers_dataset', 'BASE TABLE')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7s-BxaWSiXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d850265-ee6b-4e81-be61-9c6ddb308309"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# olist_customers_dataset.csv\n",
        "customers_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_customers_dataset.csv'\n",
        "customers_dataset = pd.read_csv(customers_dataset_url)\n",
        "\n",
        "# olist_geolocation_dataset.csv\n",
        "geolocation_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_geolocation_dataset.csv'\n",
        "geolocation_dataset = pd.read_csv(geolocation_dataset_url)\n",
        "\n",
        "# olist_order_items_dataset.csv\n",
        "order_items_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_order_items_dataset.csv'\n",
        "order_items_dataset = pd.read_csv(order_items_dataset_url)\n",
        "\n",
        "# olist_order_payments_dataset.csv\n",
        "order_payments_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_order_payments_dataset.csv'\n",
        "order_payments_dataset = pd.read_csv(order_payments_dataset_url)\n",
        "\n",
        "# olist_order_reviews_dataset.csv\n",
        "order_reviews_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_order_reviews_dataset.csv'\n",
        "order_reviews_dataset = pd.read_csv(order_reviews_dataset_url)\n",
        "\n",
        "# olist_orders_dataset.csv\n",
        "orders_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_orders_dataset.csv'\n",
        "orders_dataset = pd.read_csv(orders_dataset_url)\n",
        "\n",
        "# olist_products_dataset.csv\n",
        "products_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_products_dataset.csv'\n",
        "products_dataset = pd.read_csv(products_dataset_url)\n",
        "\n",
        "# olist_sellers_dataset.csv\n",
        "sellers_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_sellers_dataset.csv'\n",
        "sellers_dataset = pd.read_csv(sellers_dataset_url)\n",
        "\n",
        "# product_category_name_translation.csv\n",
        "product_category_name_translation_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/product_category_name_translation.csv'\n",
        "product_category_name_translation = pd.read_csv(product_category_name_translation_url)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2AQ-RVC8-EO"
      },
      "source": [
        "def mssql_olist_data_bulk_insert_dataframe(dataframe, table_name):\r\n",
        "\r\n",
        "  import numpy as np\r\n",
        "  import urllib\r\n",
        "  import sqlalchemy\r\n",
        "  from datetime import datetime\r\n",
        "  import pandas as pd\r\n",
        "  import pyodbc \r\n",
        "\r\n",
        "  connection_string = \"DRIVER={};SERVER={};DATABASE={};UID={};PWD={}\".format('ODBC Driver 17 for SQL Server',\r\n",
        "                                                                             credentials['mssql_server'],\r\n",
        "                                                                             credentials['mssql_database'],\r\n",
        "                                                                             credentials['mssql_uid'],                                   \r\n",
        "                                                                             credentials['mssql_pwd'])\r\n",
        "\r\n",
        "  batch_size = 200000\r\n",
        "  batches = np.ceil(dataframe.shape[0] / batch_size)\r\n",
        "\r\n",
        "  for i in range(int(batches)):\r\n",
        "    \r\n",
        "    if i == batches - 1:\r\n",
        "      inicio = i * batch_size\r\n",
        "      final = dataframe.shape[0]\r\n",
        "    \r\n",
        "    else:\r\n",
        "      inicio = i * batch_size\r\n",
        "      final = inicio + batch_size\r\n",
        "\r\n",
        "    temp_dataframe = dataframe.iloc[inicio: final]\r\n",
        "\r\n",
        "    params = urllib.parse.quote_plus(connection_string)\r\n",
        "    engine = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params, fast_executemany=True)\r\n",
        "\r\n",
        "    temp_dataframe.to_sql(table_name, engine, if_exists='append', index=False)\r\n",
        "\r\n",
        "    # print(f'Batch: {i+1} de {int(batches)}')\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTdTiyGLErOJ"
      },
      "source": [
        "def mssql_clear_table(table_name):\r\n",
        "\r\n",
        "  try:\r\n",
        "      conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\r\n",
        "                                SERVER = credentials['mssql_server'],\r\n",
        "                                DATABASE = credentials['mssql_database'],\r\n",
        "                                UID = credentials['mssql_uid'],\r\n",
        "                                PWD = credentials['mssql_pwd'])\r\n",
        "      \r\n",
        "\r\n",
        "      cursor = conexion.cursor()\r\n",
        "      cursor.execute(f'delete from {table_name};')\r\n",
        "\r\n",
        "      conexion.commit()\r\n",
        "      conexion.close()\r\n",
        "\r\n",
        "      \r\n",
        "  except Exception as e:\r\n",
        "      print(\"Ocorreu um erro na tentativa de conexão: \", e) "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Pnb-lmGJav"
      },
      "source": [
        "# customers_dataset\r\n",
        "mssql_clear_table('customers_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(customers_dataset, 'customers_dataset')\r\n",
        "\r\n",
        "# geolocation_dataset\r\n",
        "mssql_clear_table('geolocation_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(geolocation_dataset, 'geolocation_dataset')\r\n",
        "\r\n",
        "# order_items_dataset\r\n",
        "mssql_clear_table('order_items_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(order_items_dataset, 'order_items_dataset')\r\n",
        "\r\n",
        "# order_payments_dataset\r\n",
        "mssql_clear_table('order_payments_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(order_payments_dataset, 'order_payments_dataset')\r\n",
        "\r\n",
        "# order_reviews_dataset\r\n",
        "mssql_clear_table('order_reviews_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(order_reviews_dataset, 'order_reviews_dataset')\r\n",
        "\r\n",
        "# orders_dataset\r\n",
        "mssql_clear_table('orders_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(orders_dataset, 'orders_dataset')\r\n",
        "\r\n",
        "# products_dataset\r\n",
        "mssql_clear_table('products_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(products_dataset, 'products_dataset')\r\n",
        "\r\n",
        "# sellers_dataset\r\n",
        "mssql_clear_table('sellers_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(sellers_dataset, 'sellers_dataset')\r\n",
        "\r\n",
        "# product_category_name_translation\r\n",
        "mssql_clear_table('product_category_name_translation')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(product_category_name_translation, 'product_category_name_translation')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsw79xj_vIsg",
        "outputId": "9bbb9eec-eb4c-47e7-b276-34525f8c9848"
      },
      "source": [
        "try:\r\n",
        "  conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\r\n",
        "                            SERVER = credentials['mssql_server'],\r\n",
        "                            DATABASE = credentials['mssql_database'],\r\n",
        "                            UID = credentials['mssql_uid'],\r\n",
        "                            PWD = credentials['mssql_pwd'])\r\n",
        "\r\n",
        "\r\n",
        "  results = pd.read_sql('SELECT * FROM INFORMATION_SCHEMA.TABLES;', conexion)\r\n",
        "\r\n",
        "  for tabela in results['TABLE_NAME']:\r\n",
        "    # print(tabela)\r\n",
        "    print(tabela, pd.read_sql(f'SELECT * FROM {tabela};', conexion).shape[0])\r\n",
        "\r\n",
        "  conexion.close()\r\n",
        "   \r\n",
        "    \r\n",
        "except Exception as e:\r\n",
        "  print(\"Ocorreu um erro na tentativa de conexão: \", e)\r\n",
        "\r\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "geolocation_dataset 1000163\n",
            "order_items_dataset 112650\n",
            "order_payments_dataset 103886\n",
            "order_reviews_dataset 100000\n",
            "orders_dataset 99441\n",
            "products_dataset 32951\n",
            "sellers_dataset 3095\n",
            "product_category_name_translation 71\n",
            "customers_dataset 99441\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}